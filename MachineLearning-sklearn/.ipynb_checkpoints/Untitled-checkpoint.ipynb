{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor  # 多层线性回归\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([1, 2, 3])  # training samples \n",
    "y = np.array([1, 4, 9]) # training target\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# solver='lbfgs',  MLP的求解方法：L-BFGS 在小数据上表现较好，Adam 较为鲁棒，SGD在参数调整较优时会有最佳表现（分类效果与迭代次数）；SGD标识随机梯度下降。\n",
    "# alpha:L2的参数：MLP是可以支持正则化的，默认为L2，具体参数需要调整\n",
    "# hidden_layer_sizes=(5, 2) hidden层2层,第一层5个神经元，第二层2个神经元)，2层隐藏层，也就有3层神经网络\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5,2), random_state=1)\n",
    "clf.fit(X, y)\n",
    "print('the result of predict is:', clf.predict([1]))  # 预测某个输入对象\n",
    "\n",
    "\n",
    "#cengindex = 0\n",
    "#for wi in clf.coefs_:\n",
    "#    cengindex += 1  # 表示底第几层神经网络。\n",
    "#    print(cengindex, wi.shape, wi)  # 打印每层的权重矩阵。第1层为两个输入到5个神经元，第元到1个输出结果2层为5个神经元到2个神经元，第3层为2个神经"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
